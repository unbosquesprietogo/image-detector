# -*- coding: utf-8 -*-
"""TensorFlowImageDetector.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qz7oP1hMweafXPcaw3Ri5-p9zqlEPfmr
"""

import tensorflow_datasets as tfds
import tensorflow as tf

# Cargar el dataset sin formato supervisado
datos, metadatos = tfds.load('stanford_online_products', as_supervised=False, with_info=True)

#Imprimir los metadatos para revisarlos
metadatos

# Supongamos que los super_class_id correspondientes a las primeras cinco categorías son del 0 al 4
categorias_permitidas = [0, 1, 2, 3, 4]  # Corresponde a bicycle, cabinet, chair, coffee_maker, fan

# Filtrar el dataset de entrenamiento
datos['train'] = datos['train'].filter(lambda x: tf.reduce_any(tf.equal(x['super_class_id'], categorias_permitidas)))

# Filtrar el dataset de prueba
datos['test'] = datos['test'].filter(lambda x: tf.reduce_any(tf.equal(x['super_class_id'], categorias_permitidas)))

#Una forma de mostrar 5 ejemplos del set
tfds.as_dataframe(datos['train'].take(5), metadatos)

#Una forma de mostrar 5 ejemplos del set
tfds.as_dataframe(datos['train'].take(5), metadatos)

#Otra forma de mostrar ejemplos del set
tfds.show_examples(datos['train'], metadatos)

#Manipular y visualizar el set
#Lo pasamos a TAMANO_IMG (100x100) y a blanco y negro (solo para visualizar)
import matplotlib.pyplot as plt
import cv2

plt.figure(figsize=(20,20))

TAMANO_IMG=200

for i, ejemplo in enumerate(datos['train'].take(25)):  # `ejemplo` es un diccionario
    imagen = ejemplo['image']  # Acceder a la imagen en el diccionario
    etiqueta = ejemplo['super_class_id']  # Acceder a la etiqueta (super_class_id)
    imagen = cv2.resize(imagen.numpy(), (TAMANO_IMG, TAMANO_IMG))
    imagen = cv2.cvtColor(imagen, cv2.COLOR_BGR2GRAY)
    plt.subplot(5, 5, i+1)
    plt.xticks([])
    plt.yticks([])
    plt.imshow(imagen, cmap='gray')

#Variable que contendra todos los pares de los datos (imagen y etiqueta) ya modificados (blanco y negro, 100x100)
datos_entrenamiento = []

for i, ejemplo in enumerate(datos['train']):  # Accede al diccionario
    imagen = ejemplo['image']  # Extraer la imagen del diccionario
    etiqueta = ejemplo['super_class_id']  # Extraer la etiqueta

    # Convertir la imagen de tensor a numpy array y procesarla
    imagen = cv2.resize(imagen.numpy(), (TAMANO_IMG, TAMANO_IMG))  # Redimensionar imagen
    imagen = cv2.cvtColor(imagen, cv2.COLOR_BGR2GRAY)  # Convertir a escala de grises
    imagen = imagen.reshape(TAMANO_IMG, TAMANO_IMG, 1)  # Redimensionar a (100, 100, 1) para tener un canal

    datos_entrenamiento.append([imagen, etiqueta])  # Añadir imagen y etiqueta a la lista

#Ver los datos del primer indice
datos_entrenamiento[0]

#Ver cuantos datos tengo en la variable
len(datos_entrenamiento)

#Preparar mis variables X (entradas) y y (etiquetas) separadas

X = [] #imagenes de entrada (pixeles)
y = [] #etiquetas (perro o gato)

for imagen, etiqueta in datos_entrenamiento:
  X.append(imagen)
  y.append(etiqueta)

X

#Normalizar los datos de las X (imagenes). Se pasan a numero flotante y dividen entre 255 para quedar de 0-1 en lugar de 0-255
import numpy as np

X = np.array(X).astype(float) / 255

y

#Convertir etiquetas en arreglo simple
y = np.array(y)

X.shape

import gc
gc.collect()

#Realizar el aumento de datos con varias transformaciones. Al final, graficar 10 como ejemplo
from tensorflow.keras.preprocessing.image import ImageDataGenerator

datagen = ImageDataGenerator(
    rotation_range=30,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=15,
    zoom_range=[0.7, 1.4],
    horizontal_flip=True,
    vertical_flip=True
)

datagen.fit(X)

plt.figure(figsize=(20,8))

for imagen, etiqueta in datagen.flow(X, y, batch_size=10, shuffle=False):
  for i in range(10):
    plt.subplot(2, 5, i+1)
    plt.xticks([])
    plt.yticks([])
    plt.imshow(imagen[i].reshape(200, 200), cmap="gray")
  break

modeloCNN_AD = tf.keras.models.Sequential([
    tf.keras.layers.InputLayer(input_shape=(200, 200, 1)),  # Especificar el tamaño de la entrada
    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2, 2),
    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2, 2),
    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2, 2),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(200, activation='relu'),
    tf.keras.layers.Dense(5, activation='softmax')  # Asegúrate de que coincida con el número de clases
])

modeloCNN_AD.compile(optimizer='adam',
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])

# Commented out IPython magic to ensure Python compatibility.
from tensorflow.keras.callbacks import TensorBoard

#Cargar la extension de tensorboard de colab
# %load_ext tensorboard
#Ejecutar tensorboard e indicarle que lea la carpeta "logs"
# %tensorboard --logdir logs

#Separar los datos de entrenamiento y los datos de pruebas en variables diferentes

len(X) * .85 #20200

len(X) - 20200 #3639

X_entrenamiento = X[:20200]
X_validacion = X[20200:]

y_entrenamiento = y[:20200]
y_validacion = y[20200:]

#Usar la funcion flow del generador para crear un iterador que podamos enviar como entrenamiento a la funcion FIT del modelo
data_gen_entrenamiento = datagen.flow(X_entrenamiento, y_entrenamiento, batch_size=32)

dataset_entrenamiento = tf.data.Dataset.from_tensor_slices((X_entrenamiento, y_entrenamiento))
dataset_entrenamiento = dataset_entrenamiento.batch(32).repeat()

dataset_validacion = tf.data.Dataset.from_tensor_slices((X_validacion, y_validacion))
dataset_validacion = dataset_validacion.batch(32)

steps_per_epoch = int(np.ceil(len(X_entrenamiento) / float(32)))
validation_steps = int(np.ceil(len(X_validacion) / 32))

# Crear el callback de TensorBoard
tensorboardCNN_AD = TensorBoard(log_dir='logs/cnn_AD')

# Entrenamiento sin validación, con callback de TensorBoard
historial = modeloCNN_AD.fit(
    dataset_entrenamiento,
    epochs=150,  # Número de épocas
    steps_per_epoch=steps_per_epoch,
    callbacks=[tensorboardCNN_AD]  # Añadir el callback aquí
)

# Evaluar el conjunto de validación por separado
val_loss, val_accuracy = modeloCNN_AD.evaluate(dataset_validacion, steps=validation_steps)

with tf.summary.create_file_writer('logs/cnn_AD/validation').as_default():
    tf.summary.scalar('val_loss', val_loss, step=150)
    tf.summary.scalar('val_accuracy', val_accuracy, step=150)

print(modeloCNN_AD.summary())

modeloCNN_AD.save('products_img.h5')

modeloCNN_AD.export('saved_model_directory')

!pip install tensorflowjs

!mkdir carpeta_salida_keras

!tensorflowjs_converter --input_format keras products_img.h5 carpeta_salida

!tensorflowjs_converter --input_format=tf_saved_model --output_format=tfjs_graph_model saved_model_directory carpeta_salida_keras

# Instalar PIL (si es necesario)
!pip install Pillow

from PIL import Image
import numpy as np
import tensorflow as tf

# Cargar el modelo previamente entrenado
modeloCNN_AD = tf.keras.models.load_model('products_img.h5')  # O 'products_img.keras'

# Función para cargar y procesar la imagen
def cargar_y_procesar_imagen(ruta_imagen):
    # Cargar la imagen desde la ruta
    imagen = Image.open(ruta_imagen)

    # Redimensionar la imagen a 200x200
    imagen = imagen.resize((200, 200))

    # Convertir la imagen a escala de grises (si es necesario)
    imagen = imagen.convert('L')  # L convierte a escala de grises

    # Convertir la imagen a un array numpy y normalizarla
    imagen_array = np.array(imagen).astype('float32') / 255.0

    # Redimensionar para cumplir con la forma de entrada del modelo: (1, 150, 150, 1)
    imagen_array = np.expand_dims(imagen_array, axis=-1)  # Añadir el canal
    imagen_array = np.expand_dims(imagen_array, axis=0)   # Añadir el lote

    return imagen_array

# Función para realizar la predicción
def predecir_imagen(modelo, imagen_preprocesada):
    # Realizar la predicción
    prediccion = modelo.predict(imagen_preprocesada)

    # Obtener la clase con mayor probabilidad
    clase_predicha = np.argmax(prediccion)

    return clase_predicha

# Probar con una imagen del servidor local
ruta_imagen = 'bici.jpg'  # Especifica la ruta de tu imagen
imagen_preprocesada = cargar_y_procesar_imagen(ruta_imagen)
clase_predicha = predecir_imagen(modeloCNN_AD, imagen_preprocesada)

# Etiquetas del modelo (modifica según las clases en tu modelo)
etiquetas = ['Bicycle', 'Cabinet', 'Chair', 'Coffee Maker', 'Fan']
print(f"El modelo predice que la imagen es: {etiquetas[clase_predicha]}")

# Probar con una imagen del servidor local
ruta_imagen = 'armario.jpg'  # Especifica la ruta de tu imagen
imagen_preprocesada = cargar_y_procesar_imagen(ruta_imagen)
clase_predicha = predecir_imagen(modeloCNN_AD, imagen_preprocesada)

# Etiquetas del modelo (modifica según las clases en tu modelo)
etiquetas = ['Bicycle', 'Cabinet', 'Chair', 'Coffee Maker', 'Fan']
print(f"El modelo predice que la imagen es: {etiquetas[clase_predicha]}")

# Probar con una imagen del servidor local
ruta_imagen = 'silla.jpg'  # Especifica la ruta de tu imagen
imagen_preprocesada = cargar_y_procesar_imagen(ruta_imagen)
clase_predicha = predecir_imagen(modeloCNN_AD, imagen_preprocesada)

# Etiquetas del modelo (modifica según las clases en tu modelo)
etiquetas = ['Bicycle', 'Cabinet', 'Chair', 'Coffee Maker', 'Fan']
print(f"El modelo predice que la imagen es: {etiquetas[clase_predicha]}")

# Probar con una imagen del servidor local
ruta_imagen = 'cafe.jpg'  # Especifica la ruta de tu imagen
imagen_preprocesada = cargar_y_procesar_imagen(ruta_imagen)
clase_predicha = predecir_imagen(modeloCNN_AD, imagen_preprocesada)

# Etiquetas del modelo (modifica según las clases en tu modelo)
etiquetas = ['Bicycle', 'Cabinet', 'Chair', 'Coffee Maker', 'Fan']
print(f"El modelo predice que la imagen es: {etiquetas[clase_predicha]}")

# Probar con una imagen del servidor local
ruta_imagen = 'ven.jpg'  # Especifica la ruta de tu imagen
imagen_preprocesada = cargar_y_procesar_imagen(ruta_imagen)
clase_predicha = predecir_imagen(modeloCNN_AD, imagen_preprocesada)

# Etiquetas del modelo (modifica según las clases en tu modelo)
etiquetas = ['Bicycle', 'Cabinet', 'Chair', 'Coffee Maker', 'Fan']
print(f"El modelo predice que la imagen es: {etiquetas[clase_predicha]}")

import shutil

# Nombre de la carpeta que deseas comprimir
carpeta_a_comprimir = 'carpeta_salida_keras'

# Nombre del archivo ZIP de salida
nombre_archivo_zip = 'salida_comprimida_keras'

# Comprimir la carpeta en un archivo ZIP
shutil.make_archive(nombre_archivo_zip, 'zip', carpeta_a_comprimir)

print(f"Carpeta '{carpeta_a_comprimir}' comprimida como '{nombre_archivo_zip}.zip'")